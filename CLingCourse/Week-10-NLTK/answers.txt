#1 Library Comparison

#Sentence segmentation

1. NLTK tokenized numbers separately. See diff below:

-1. Be passionate and focused.
+1.
+Be passionate and focused.

2. Mistakes in contractions. See diff below:

-4. Let your employees do their jobs.
-Mayer, who has an M.S. in computer science, misses writing mobile apps and code--but recognizes that it's not her responsibility anymore.
+4.
+Let your employees do their jobs.
+Mayer, who has an M.S.
+in computer science, misses writing mobile apps and code--but recognizes that it's not her responsibility anymore.

To sum up, openNLP did a better job in sentence segmentation here.

#Tokenization
 
Well, openNLP was not that excellent here... Some mistakes:

1. openNLP separated .html from the address:
-Available at http : //www.inc.com/cameron-albert-deitch/marissa-mayer-management-tips-ad-week.html 
+Available at http ://www.inc.com/cameron-albert-deitch/marissa-mayer-management-tips-ad-week .html

2. "--" not separated
-It 's a busy time for Yahoo CEO Marissa Mayer -- but you would n't know it to hear her talk .
+It 's a busy time for Yahoo CEO Marissa Mayer--but you would n't know it to hear her talk .

3. Not sure if it's a mistake, but different kinds of brackets in openNLP and NLTK:
" When you 're coming into a company , and you know have to do a transformation , what you really want to do is look at the company and say , 'Okay , here are the parts that the company does well . How do we get those genes to hyper-express ? The genes that are getting in the way , how do you turn those off ? ' "
`` When you 're coming into a company , and you know have to do a transformation , what you really want to do is look at the company and say , 'Okay , here are the parts that the company does well .
How do we get those genes to hyper-express ?
The genes that are getting in the way , how do you turn those off ? ' ''


#2 It's Cloudy

- The output of the function tells pretty much about the input_file genre, but not the entire text idea. 
- I think that in this particular case names are not to be filtered out, because some famous names can tell us more about the period when the speech was made. Also, some words could be both names and have other meanings. Therefore, it is better not to filter them out.
Another thing that I've noticed: words like America, American are filtered out in the sample input and output because we firstly lowercase all words and then check if they are in "words" corpora. I don't think it is much needed to check with the "words" corpora at all. Maybe some unusual words are distinctive specifically for this time or for this particular speaker, so they can also tell a lot of interesting things.
- I'd add modal verbs to the stopwords.
- Yes! It took me almost 2 hours to figure out that (u'u', 10) that is among most common for Clinton, Cleveland and Obama is meant to be "us".

Some observations:
Words that tend to repeat are: us, people, nation. 
In the 18th century the most common words are, of course, very archaic words: oath, shall, etc.
Interesting that starting from the 90s words such as "must" and "work" are common. In my humble opinion this describes the consumerism culture in U.S.A. Of course, it's just a guess :)


Outputs (every 100 years):

get_keywords('1793-Washington.txt')
[(u'shall', 3), (u'oath', 2), (u'Constitution', 1), (u'office', 1), (u'distinguished', 1), (u'knowingly', 1), (u'high', 1), (u'sense', 1), (u'proper', 1), (u'endeavor', 1), (u'voice', 1), (u'subject', 1), (u'willingly', 1), (u'confidence', 1), (u'united', 1)]

get_keywords('1893-Cleveland.txt')
[(u'people', 22), (u'Government', 10), (u'u', 10), (u'public', 8), (u'every', 7), (u'service', 7), (u'support', 7), (u'national', 6), (u'upon', 6), (u'power', 5), (u'confidence', 5), (u'free', 4), (u'must', 4), (u'may', 4), (u'shall', 4)]

get_keywords('1993-Clinton.txt')
[(u'world', 20), (u'must', 18), (u'u', 13), (u'people', 12), (u'change', 9), (u'new', 9), (u'today', 7), (u'time', 7), (u'work', 6), (u'nation', 5), (u'idea', 5), (u'done', 5), (u'Let', 5), (u'fellow', 5), (u'let', 4)]

get_keywords('2009-Obama.txt')
[(u'u', 23), (u'nation', 12), (u'new', 11), (u'must', 8), (u'every', 8), (u'work', 7), (u'world', 7), (u'le', 6), (u'cannot', 6), (u'common', 6), (u'people', 6), (u'let', 5), (u'today', 5), (u'day', 5), (u'generation', 5)]

#3 Generate a Dictionary

 I think this function could be used for research before writing a pattern, especially when it is a pattern for prepositions. And I am actually thinking to reverse this function and to use it in my diploma paper in order to get the concordance.


